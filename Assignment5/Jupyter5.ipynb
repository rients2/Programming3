{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import mean\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.stat import Correlation\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/commons/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/17 14:33:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local[16]')\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/data/dataprocessing/interproscan/all_bacilli.tsv', sep=r'\\t', header=False)#.select('col1','col2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF('Protein_accession','Sequence_MD5_digest','Sequence_length','Analysis','Signature_accession',\n",
    "             'Signature_description','Start_location','Stop_location','Score','Status','Date','INT_accession',\n",
    "             'INT_description','GO_annotations','Pathways_annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yoink\n",
    "def explain_func(data):\n",
    "    return data._sc._jvm.PythonSQLUtils.explainString(data._jdf.queryExecution(), 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many distinct protein annotations are found in the dataset? I.e. how many distinc InterPRO numbers are there?\n",
    "2. How many annotations does a protein have on average?\n",
    "3. What is the most common GO Term found?\n",
    "4. What is the average size of an InterPRO feature found in the dataset?\n",
    "5. What is the top 10 most common InterPRO features?\n",
    "6. If you select InterPRO features that are almost the same size (within 90-100%) as the protein itself, what is the top10 then?\n",
    "7. If you look at those features which also have textual annotation, what is the top 10 most common word found in that annotation?\n",
    "8. And the top 10 least common?\n",
    "9. Combining your answers for Q6 and Q7, what are the 10 most commons words found for the largest InterPRO features?\n",
    "10. What is the coefficient of correlation ($R^2$) between the size of the protein and the number of features found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#1\n",
    "quest1 = df.select('INT_accession').distinct().count()\n",
    "quest1_explain = explain_func(df.select('INT_accession').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#2. \n",
    "quest2 = df.select('Protein_accession').count() / df.select('Protein_accession').distinct().count()\n",
    "quest2_explain = explain_func(df.select('Protein_accession')), explain_func(df.select('Protein_accession'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#3\n",
    "quest3df = df[df.GO_annotations.contains('GO')]\n",
    "quest3 = quest3df.groupby('GO_annotations').count().orderBy(\"count\", ascending=False).first()[0]\n",
    "quest3_explain = explain_func(quest3df.groupby('GO_annotations').count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#4\n",
    "# Its either this:\n",
    "quest4df = df.withColumn('lenght', ( df['Stop_location'] - df['Start_location'] ))\n",
    "quest4 = quest4df.select(mean('lenght')).collect()[0][0]\n",
    "quest4_explain = explain_func(quest4df.select(mean('lenght')))\n",
    "# Or this:\n",
    "# quest4 = df.select(mean ('Sequence_length')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#5\n",
    "quest5df = df[df.INT_accession.contains('IPR0')]\n",
    "quest5_list = quest5df.groupby('INT_accession').count().orderBy(\"count\", ascending=False).take(10)\n",
    "quest5_explain = explain_func(quest5df.groupby('INT_accession').count().orderBy(\"count\", ascending=False))\n",
    "quest5 = []\n",
    "for nr in range(0,len(quest5_list)):\n",
    "    quest5.append(quest5_list[nr][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#6\n",
    "quest6df = quest5df.withColumn('lenght', ( df['Stop_location'] - df['Start_location'] ) / df['Sequence_length'])\n",
    "quest6df2 = quest6df[quest6df['lenght'] > 0.9]\n",
    "quest6_list = quest6df2.groupby('INT_accession').count().orderBy(\"count\", ascending=False).take(10)\n",
    "quest6_explain = explain_func(quest6df2.groupby('INT_accession').count().orderBy(\"count\", ascending=False))\n",
    "quest6 = []\n",
    "for nr in range(0,len(quest6_list)):\n",
    "    quest6.append(quest6_list[nr][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#7\n",
    "# Thank you stackoverflow https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe \n",
    "quest7df = df[df.INT_accession.isin('-') == False]\n",
    "quest7_list = quest7df.withColumn('word', f.explode(f.split(f.col('INT_description'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .take(10)\n",
    "quest7_explain = explain_func(quest7df.withColumn('word', f.explode(f.split(f.col('INT_description'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False))\n",
    "quest7 = []\n",
    "for nr in range(0,len(quest7_list)):\n",
    "    quest7.append(quest7_list[nr][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#8\n",
    "quest8_list = quest7df.withColumn('word', f.explode(f.split(f.col('INT_description'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=True)\\\n",
    "    .take(10)\n",
    "quest8_explain = explain_func(quest7df.withColumn('word', f.explode(f.split(f.col('INT_description'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=True))\n",
    "quest8 = []\n",
    "for nr in range(0,len(quest8_list)):\n",
    "    quest8.append(quest8_list[nr][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#9\n",
    "quest9_list = quest6df2.withColumn('word', f.explode(f.split(f.col('INT_description'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .take(10)\n",
    "quest9_explain = explain_func(quest6df2.withColumn('word', f.explode(f.split(f.col('INT_description'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False))\n",
    "quest9 = []\n",
    "for nr in range(0,len(quest9_list)):\n",
    "    quest9.append(quest9_list[nr][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest10df = quest6df\n",
    "quest10df = quest10df.withColumn('Sequence_length', quest10df['Sequence_length'].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#10\n",
    "quest10df = quest6df\n",
    "quest10df = quest10df.withColumn('Sequence_length', quest10df['Sequence_length'].cast(\"int\"))\n",
    "quest10df2 = quest7df.groupby('INT_accession').count()\n",
    "quest10df = quest10df.join(quest10df2,'INT_accession')\n",
    "quest10_explain = explain_func(quest10df)\n",
    "quest10 = quest10df.corr('Sequence_length', 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the csv\n",
    "output = {'question_nr' : range(1,11),\n",
    " 'answers' : [quest1, quest2, quest3, quest4, quest5, quest6, quest7, quest8, quest9, quest10],\n",
    " 'explain' : [quest1_explain, quest2_explain, quest3_explain, quest4_explain, quest5_explain, quest6_explain, quest7_explain, quest8_explain, quest9_explain, quest10_explain],\n",
    "}\n",
    "output = pd.DataFrame(output)\n",
    "output.to_csv('/homes/rdalstra/Documents/Programming3/Assignment5/output/answers.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca1b872015604588654afab8889c327752f46265fe55163fa640f2d3ca6aaea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
